{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get NOAA ghcn/daily/by_year into parquet on S3\n",
    "\n",
    "**How to process the new NOAA data (from Yoav)**\n",
    "\n",
    "*Input:* A dataframe with columns: `Station, Year, day-in-year, measurement type, value`\n",
    "1. Translate Dataframe into RDD of rows\n",
    "2. Map into key-value RDD with the format `key=(Station,year, measurement)  value=[(day-of-year, value)]`\n",
    "3. Reduce by key: take the union\n",
    "4. Results in an RDD of the form: `key=(Station,year, measurement) value=[(....),(....)  ]`\n",
    "5. Translate the RDD into a dataframe: map value list into a 365 array, pack into bytearray"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup and imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Startup import *\n",
    "from pyspark.sql.types import *\n",
    "sc.stop()\n",
    "sc = SparkContext(appName=\"CSV2Parquet\")\n",
    "sqlContext = SQLContext(sc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get the .csv files from NOAA s3 bucket and put into data fame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "startYear = 1763  # data starts in 1763\n",
    "endYear = 2020  # data (currently ends in 2020)\n",
    "\n",
    "# set schema for import from csv\n",
    "schemaString = \"id year_date element data_value\"\n",
    "schema = StructType([StructField(field_name, StringType(), True) for field_name in schemaString.split()])\n",
    "\n",
    "# the s3 bucket noaa-ghcn-pds/csv/ contains all of the observations from 1763 to the present organized in .csv files\n",
    "# loop through all years \n",
    "allYears = np.arange(startYear,endYear+1)\n",
    "for yr in allYears:\n",
    "    fn = \"s3://noaa-ghcn-pds/csv/\" + str(yr) + \".csv\"\n",
    "    dt = sc.textFile(fn).map(lambda l: l.split(\",\")).map(lambda p: ([x.strip() for x in p[0:4]]))\n",
    "    schemaDT = sqlContext.createDataFrame(dt, schema)\n",
    "    if yr == allYears[0]:\n",
    "        df = schemaDT\n",
    "    else:\n",
    "        df = df.union(schemaDT)\n",
    "        \n",
    "# register table for querying\n",
    "df.createOrReplaceTempView(\"ghcnd\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Query for data, and specify if the year is a leap year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# cast values to appropriate types and include whether year is leap year or not\n",
    "qry = \"\"\"\n",
    "SELECT id AS Station,\n",
    "       CAST(SUBSTRING(year_date, 1, 4) AS SMALLINT) AS Year,\n",
    "       DAYOFYEAR(TO_DATE(year_date,'yyyyMMdd')) AS Day,\n",
    "       NOT ISNULL(TO_DATE(CONCAT(SUBSTRING(year_date, 1, 4), '0229'),'yyyyMMdd')) AS isleapyear,\n",
    "       element AS Measurement,\n",
    "       CAST(data_value AS FLOAT) AS Value\n",
    "FROM ghcnd\"\"\"\n",
    "raw_data = sqlContext.sql(qry)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get the data into [Station, Year, Measurement, Value by Day of Year] format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# define function for mapping (day-of-year, value) pairs into 365-day array\n",
    "def putIntoArray(x):\n",
    "    arr = [None] * 366  # initialize list\n",
    "    for pair in x[1]:\n",
    "        arr[pair[0]-1] = pair[1]  # set the (dayOfYear-1)-th entry to the corresponding value\n",
    "    if x[0][3]:\n",
    "        del arr[59]  # delete feb 29 for leap year\n",
    "    else:\n",
    "        del arr[365]  # delete day 366 for non leap year\n",
    "        print('toss last day')\n",
    "    return (x[0][0], x[0][1], x[0][2], arr)\n",
    "\n",
    "# 1. Translate Dataframe into RDD of rows\n",
    "arr_rdd = (raw_data.rdd\n",
    "           # 2. Map into key-value RDD with the format `key=(Station,year, measurement)  value=[(day-of-year, value)]`\n",
    "           .map(lambda x: ((x[0],x[1],x[4],x[3]),[(x[2],x[5])]))\n",
    "           # 3. Reduce by key: take the union\n",
    "           # 4. Results in an RDD of the form: `key=(Station,year, measurement) value=[(....),(....)  ]`\n",
    "           .reduceByKey(lambda a, b: a + b)\n",
    "           # 5. Translate the RDD into a dataframe: map value list into a 365 array, pack into bytearray\n",
    "           .map(putIntoArray))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make a dataframe from the RDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "schemaString = \"Station Year Measurement Values\"\n",
    "typeList = [StringType(), IntegerType(), StringType(), ArrayType(DoubleType())]\n",
    "schema = StructType([StructField(field_name, typeList[i], True) for i,field_name in enumerate(schemaString.split())])\n",
    "arr_df = sqlContext.createDataFrame(arr_rdd,schema)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Write to a .parquet file on s3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "arr_df.write.parquet(\"s3://philipp-ghcnd/GHCNDby_year.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test by loading data from S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "s3Data = sqlContext.sql(\"SELECT * FROM parquet.`s3://philipp-ghcnd/GHCNDbyYear.parquet`\")\n",
    "s3Data.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
